{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdMTmAw0_U6t"
   },
   "source": [
    "# Homework 4 - Code Search using Deep Learning Bi-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uplu6Pe27p09"
   },
   "source": [
    "Modern search engines are capable of working with more than just text: images, videos, audio, songs, ... the list goes on and on! In Homework 4, you'll develop an IR system that works in two modalities: text queries and _code_ documents. Code may be written in text characters (it's human readable, unlike image files) but queries like ``python function to test if file exists`` may look nothing like the code that matches the query's intent.\n",
    "\n",
    "To bridge these two languages, we'll use a relative recent technique known as a *bi-encoder* model to convert text queries and code documents into the same representation space. If you recall from class, our tf-idf vectorizer was an encoderâ€”-it would convert some text query into a high-dimensional vector, _encoding_ the text into some numeric space. Here, we'll use deep learning encoders that also produce numeric vectors. However, unlike our tf-idf vectors which are large (the size of the vocabulary) and sparse (most queries don't have most words), our deep learning-produced vectors will be relatively small (hundreds of dimensions) and dense (all dimensions have a non-zero value). \n",
    "\n",
    "For text, most of the common deep learning models are similar to the **BERT** model we talked about in class during the deep learning week. If you recall, this was the model that was trained using a MadLibs-style approach where it's shown most of a sentence and asked to fill in the blanks for a few words that have been removed. This type of training lets the model ultimately produce a dense vector representation for the \"meaning\" of a text input--essentially acting as a text encoder. If you're curious, there are a few [good](https://huggingface.co/blog/bert-101) [tutorials](https://wandb.ai/mukilan/BERT_Sentiment_Analysis/reports/An-Introduction-to-BERT-And-How-To-Use-It--VmlldzoyNTIyOTA1) on what this model is all about, though you don't need to know it for the purposes of this assignment. The most relevant thing to know is that other folks have adapted this approach to produce other models that can encode the meanings of other kinds of input, _like code_.\n",
    "\n",
    "<img src=\"https://weaviate.io/img/blog/cross-encoders/bi-encoder.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "The bi-encoder architecture (shown above for two _sentence_ inputs) use two of these deep learning encoder models to compare the encodings of each model's input in some way. Here, we'll compare an encoded query and an encoded-code-document to measure the IR _relevance_ of that document for the given query. The bi-encoder setup lets us bridge between the two languages by _training the model to adjust its representations_. The key idea here is that both encoders know how to represent each of their respective inputs (they've already been pre-trained to do this) and by further training, we can get the encoders to adjust how they represent things so that representations for very relevant query-document papers look similar and representations for non-relevant pairs look dissimilar.\n",
    "\n",
    "### Homework summary\n",
    "\n",
    "This homework provides a substantial amount of code to get you started and has you focus on just a few implementation pieces.\n",
    "\n",
    "* **Part 1** of the homework will be training the bi-encoder model. You will need a GPU for this, so you'll want to run this on a Great Lakes machine with a GPU. We've done testing and set the hyperparameters below so that it should run quickly enough. _But_ it will still take an hour to complete training once you get it up and running. Once you have the model trained, you'll export the query-document relevance scores from the bi-encoder to a file for use later.\n",
    "\n",
    "* **Part 2** will have you learning how to do Learning to Rank (L2R) again and incorporating the bi-encoder's predictions. For this part, you will need to have access to Java to run Pyterrier. However, you will not need a GPU and this part can be run either on a non-GPU Great Lakes machine or on your local machine. You will still need access to the outputs of the bi-encoder which are in a file, so if you work locally, you'll need to copy those still. Part 2 will show you how to set up new pipelines that make use of precomputed features.\n",
    "\n",
    "For both parts, we've put in extensive comments throughout so you can get a sense of what is happening and why. You don't need to understand all of the code, but it will be helpful to try to understand the general flow of things and how you train these kinds of models at a high level (possibly relating to questions on the final exam).\n",
    "\n",
    "### Implementation Notes \n",
    "\n",
    "This notebook contains a lot of code that is intended to expose you to various deep learning concepts _but_ where you don't need to modify it. To streamline the process, we've put even more low-level code into two files that you don't need to read: \n",
    "\n",
    "- **edited_roberta.py** contains the pre-trained RoBERTa model architecture.\n",
    "- **run_classifiers.py** contains the some helpful scripts for running things and loading data.\n",
    "\n",
    "If you've never seen PyTorch code, that's ok (and expected!). We've added detailed instructions on what you need to know or learn in each step. Your pytorch code will be very (_very_) minimal, but we do want you to try a few things. There are also many good tutorials out there for pytorch _but_ these tutorial cover more information than what is needed too, so please read judiciously so you don't feel overwhelmed. If you do decide to read more, perhaps one general place to start is tutorials on using PyTorch for [logistic regression](https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be).\n",
    "\n",
    "\n",
    "### What to do\n",
    "\n",
    "For Part 1, we've marked your parts with TODO. To get a sense of what and why you're doing something, be sure to read the nearby parts of the notebook for context. \n",
    "\n",
    "In general, you will need a GPU to run this. However, we've provided a very tiny input file `data/train_10.txt` that has only 10 training examples. Training only on this file will produce horrible results. _However_ it is small enough that you can probably run the code on a CPU to start debugging, though this may take a lot of memory. Worst case, you can start and debug this file on a Great Lakes machine (without a GPU) and once you think it's working, switch to a GPU.\n",
    "\n",
    "\n",
    "### Learning goals for Part 1 of Homework 4:\n",
    "* Expose you to how to run and train deep learning systems\n",
    "* Gain familiarity with bi-encoder models for IR with different modalities\n",
    "* Learn how to read and import data for training bi-encoder models\n",
    "* Learn how to export relevance predictions from deep learning models for use in IR ranking systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20747,
     "status": "ok",
     "timestamp": 1670369932051,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "-yCyrRwKzAQM",
    "outputId": "08d36d49-17c1-46c9-aa27-1bed76a268a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboardX in /home/yianchu/.local/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorboardX) (1.20.3)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/yianchu/.local/lib/python3.9/site-packages (from tensorboardX) (3.20.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-terrier in /home/yianchu/.local/lib/python3.9/site-packages (0.8.1)\n",
      "Requirement already satisfied: more-itertools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (8.10.0)\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.5.4)\n",
      "Requirement already satisfied: scipy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (1.7.1)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (4.62.3)\n",
      "Requirement already satisfied: ir-measures>=0.2.0 in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.3.1)\n",
      "Requirement already satisfied: pandas in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (1.3.4)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (2.26.0)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: deprecation in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (2.1.0)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (1.20.3)\n",
      "Requirement already satisfied: pyjnius~=1.3.0 in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (1.3.0)\n",
      "Requirement already satisfied: chest in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: statsmodels in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (0.12.2)\n",
      "Requirement already satisfied: matchpy in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: jinja2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: wget in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: sklearn in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.0)\n",
      "Requirement already satisfied: dill in /home/yianchu/.local/lib/python3.9/site-packages (from python-terrier) (0.3.6)\n",
      "Requirement already satisfied: typish>=1.7.0 in /home/yianchu/.local/lib/python3.9/site-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.6.3)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.1 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.0.2)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.5)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.1)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (3.1.4)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir-datasets>=0.3.2->python-terrier) (6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.2.1)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.2 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-measures>=0.2.0->python-terrier) (0.5.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/yianchu/.local/lib/python3.9/site-packages (from ir-measures>=0.2.0->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: six>=1.7.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: cython in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier) (0.29.24)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier) (3.2)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/yianchu/.local/lib/python3.9/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: packaging in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from deprecation->python-terrier) (21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from jinja2->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /home/yianchu/.local/lib/python3.9/site-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging->deprecation->python-terrier) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->python-terrier) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->python-terrier) (2021.3)\n",
      "Requirement already satisfied: scikit-learn in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sklearn->python-terrier) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn->sklearn->python-terrier) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from statsmodels->python-terrier) (0.5.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/yianchu/.local/lib/python3.9/site-packages (4.23.1)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/yianchu/.local/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/yianchu/.local/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yianchu/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX\n",
    "!pip install python-terrier\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "ok",
     "timestamp": 1670369946165,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "6aHjlrZNyxtQ"
   },
   "outputs": [],
   "source": [
    "from edited_roberta import *\n",
    "from run_classifier import evaluate, load_and_cache_examples, accuracy, set_seed\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW,RobertaConfig,RobertaTokenizer)\n",
    "from utils import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHCGPXvOyxtT"
   },
   "source": [
    "# Defining our Bi-Encoder model\n",
    "\n",
    "In the following block, we'll create our neural network bi-encoder model. To do so, we'll define a new class `CodeSearchBiencoderModel` that contains the two encoders, which are the encoder for query and the encoder for code. \n",
    "\n",
    "In pytorch, neural networks are defined by specifying their _parameters_ (the things that get updated during training) and a `forward` function that determines how to turn the inputs into outputs. For our bi-encoder, we'll need to fill these in as follows:\n",
    "\n",
    "* Specify the two encoders in the `__init__` function as fields of the class (e.g., `self.x = 1` makes `x` a field of the object), which will tell PyTorch that we'll be updating their parameters during training\n",
    "* Write the `forward` function so that we...\n",
    "  * encode the query as a vector\n",
    "  * encode the code-document as a vector\n",
    "  * compute the cosine similarity of the two vectors (where 1 is relevant, 0 is not-relevant)\n",
    "  \n",
    "We'll detail these next.\n",
    "\n",
    "## Creating the encoders\n",
    "\n",
    "How do we instantiate an encoder? There are two steps. First we need to figure out what is the _architecture_ of the model. This defines things like how many layers are in a neural network and how the layers are connected. In our case, _both_ of our encoders will use the RoBERTa architecture; as you might have guessed, RoBERTa is related to BERT and just has slightly different tweaks. There are [many BERT variants](https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23) and for the purposes of this excise, you can safely think of RoBERTa as the same as BERT.\n",
    "\n",
    "Second, once we have our model architecture, we need to specify which parameters we'll start with. You can think of the difference between the model architecture and parameters as if you were specifying a meal: The architecture is a bit like specifying the plates/bowls/container based on what kind of food you want and the parameters are like filling the container with a specific kind of food. There are many pre-trained sets of parameters for architectures, so a neural network starts with some existing knowledge of certain kinds of things (e.g., what human  language looks like,  what programming languages look like, or how to classify images). In neural network land, the [Huggingface Model Repository](https://huggingface.co/models) is a common place to look for parameters that people have shared with others.\n",
    "\n",
    "Returning to our IR problem, in our setting (conveniently), both of our encoders will use the same architecture: `RobertaModel`. If you _really_ want to know more, the code for this is provided in `edited_roberta.py`, but many end-users of these models (like us) will never need to look at this kind of code--and you certainly don't to complete this homework!\n",
    "\n",
    "## Writing the forward function \n",
    "\n",
    "The `forward` function defines how the neural network goes from inputs to outputs. In our case, we're going to feed the different inputs (query and document) to separate encoders and then compare the outputs. \n",
    "\n",
    "\n",
    "Your task here:\n",
    "1. Define the two encoders(query encoder and code_encoder) which are RobertaModel loaded from edit_roberta.py. You should two arguments when creating the instance of the Roberta Model, which are config and add_pooling_layer(equal to False).\n",
    "2. define and calculate the cosine similarity between two embeddings.\n",
    "3. define the loss function and loss.\n",
    "\n",
    "### TODOs in this block are worth 20 points total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1670371264958,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "IGpFmW_vyxtW"
   },
   "outputs": [],
   "source": [
    "class CodeSearchBiencoderModel(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        ## TODO: \n",
    "        # Fill in the following parts where you specify each encoder's architecture. \n",
    "        # You'll need to pass in \"config\" as an argument to the architecture's constructor\n",
    "        # so it knows how to set things up.\n",
    "        #\n",
    "        # NOTE 1: Notice that we haven't specified the *parameters* here, just the architecture.\n",
    "        # We'll fill in the parameters later\n",
    "        #\n",
    "        # NOTE 2: If you were ever curious how to do other kinds of non-text IR (e.g., images),\n",
    "        # this part is where you'd specify a different kind of encoder architecture, such as\n",
    "        # ResNet50 for encoding images. The rest of the code for this class would be mostly the same!\n",
    "        # (The one caveat is that both models need to produce vector representations of the same size)\n",
    "        \n",
    "        self.query_encoder = RobertaModel(config)\n",
    "        self.code_encoder = RobertaModel(config)\n",
    "\n",
    "        # This is our loss function that determines how \"good\" our model's output is.\n",
    "        # We'll use this in the forward() function to evaluate the model's outputs and\n",
    "        # then return the predictions and loss.        \n",
    "        self.loss_fn = BCEWithLogitsLoss()\n",
    "        \n",
    "        # This will initialize weights and apply final processing\n",
    "        self.post_init()\n",
    " \n",
    "    def forward(\n",
    "        self,\n",
    "        query_token_ids: Optional[torch.LongTensor] = None,\n",
    "        code_token_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "       \n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "        outputs = self.query_encoder(\n",
    "            query_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            \n",
    "        )\n",
    "        query_emb = outputs[0][:, 0, :]\n",
    "        \n",
    "        outputs_code = self.code_encoder(\n",
    "            code_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "           \n",
    "        )\n",
    "        code_emb = outputs_code[0][:, 0, :]\n",
    "\n",
    "        # TODO: using the cosine_similarity function (imported above),\n",
    "        # compute the similarity of the query and code embeddings.\n",
    "        cosine_sim = cosine_similarity(query_emb, code_emb)\n",
    "                       \n",
    "        # TODO: use the self.loss_fn (our loss function) to measure how good/bad\n",
    "        # the predictions are. This function will return a value you should \n",
    "        # call \"loss\". You can see how to call the function here\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "        # \n",
    "        # NOTE: The \"labels\" input to this function is the ground truth\n",
    "        # relevance scores (labels) for each input. You'll want to compare\n",
    "        # the cosine similarities with these labels when calling the function\n",
    "        #\n",
    "        # NOTE 2: There are many kinds of loss functions so understanding how\n",
    "        # to call them and which order the arguments go in is important\n",
    "        loss = self.loss_fn(cosine_sim, labels)\n",
    "\n",
    "        # Finally, let's return some output! Pytorch has provided\n",
    "        # some structure for us to say what-is-what in the output\n",
    "        # values. We'll return our loss (how \"bad\" the model's prediction was)\n",
    "        # and the logits, which is our predictions. \n",
    "        #\n",
    "        # You don't need to worry about the other two outputs.\n",
    "        #\n",
    "        # NOTE: Normally, we'd be passing the cosine similarity through\n",
    "        # some non-linear function (like a sigmoid!) to get \"logits\" as\n",
    "        # our output values. However, in a bi-encoder, often you just \n",
    "        # return the cosine similarity as the logits, so the name is wrong\n",
    "        # but the value is what's expected. Later on, when we access the\n",
    "        # \"logits\" part of the output, remember these are the query-doc\n",
    "        # cosine similarity scores\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cosine_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ9BTUDRyxta"
   },
   "source": [
    "# Configure the Models and Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBMCGRsbyxtb"
   },
   "source": [
    "Training deep learning models often involves _lots_ of hyperparameter decisions--not to mention a bunch of seemingly-random bookkeeping options for where and when to save things. We have defined these all for you (yay) but it's worth at least looking through to see what kinds of decisions you'll need to make. Most importantly, we've specified the number of epochs and the batch size (more on that later) so that the model trains quickly.\n",
    "\n",
    "You will eventually need to edit the input file name to use the full dataset. Everything else can stay the same, though you're welcome to try changing some things and seeing what happens once you've completed the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1670371269118,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "2CIf_Jwvyxtc"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Where to save things\n",
    "        self.data_dir = './data'\n",
    "        self.model_type = 'roberta'\n",
    "        self.model_name_or_path = 'microsoft/codebert-base'\n",
    "        self.task_name = 'codesearch'\n",
    "        self.output_dir = './models'\n",
    "        self.output_mode = 'codesearch'\n",
    "\n",
    "        # These are going to be your most common hyperparameters to change.\n",
    "        # If you want to do deep learning stuff, it's worth learning a bit\n",
    "        # about what they are and what they do.\n",
    "        self.train_batch_size = 64\n",
    "        self.eval_batch_size = 64\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 1e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3 # NOTE: Change this to 1 if debugging so it runs faster\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.n_gpu = 1\n",
    "        self.no_cuda = False\n",
    "\n",
    "        # These are mostly configuration options for which pieces to run\n",
    "        self.config_name = \"\"\n",
    "        self.tokenizer_name = \"\"\n",
    "        self.cache_dir = \"\"\n",
    "        self.max_seq_length = 200\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.do_predict = False\n",
    "        self.evaluate_during_training = False\n",
    "        self.do_lower_case = False\n",
    "\n",
    "        # How often we save things\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 1000\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Ignore all of these\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.local_rank = -1\n",
    "        self.server_ip = \"\"\n",
    "        self.server_port = \"\"\n",
    "        \n",
    "        # Input and output files.\n",
    "        #\n",
    "        # TODO: Change the training file to train_300k.txt when ready\n",
    "        #\n",
    "        self.train_file = \"train_300k.txt\" # CHANGE ME WHEN READY TO TRAIN!!!!!\n",
    "        self.dev_file = \"valid.txt\"\n",
    "        self.test_file = \"test_data.txt\"\n",
    "        self.pred_model_dir = './models/checkpoint-best'\n",
    "        self.test_result_dir = './results/'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhEA1uuFyxtg"
   },
   "source": [
    "In the following blocks, we are starting to train our model. Here we will firstly define the train function.\n",
    "\n",
    "In the train function, we will define the procedure of training the model, the main steps are:\n",
    "1. Define the dataloader (to do). You should use the function DataLoader(). Three arguments are required for you to input, which are dataset, batch_size, sampler.\n",
    "2. set the gradient to zero and train the model using back propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmum2QNcyxth"
   },
   "source": [
    "# Define the training process\n",
    "\n",
    "Let's see how the training procedure works! The code block below specifies how we'll train our model. There's one part for you to fill in that loads the data using the `DataLoader` class. The rest is helpful to understand how \n",
    "\n",
    "### TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1670371312584,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "tVHuPGIhyxtj"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, optimizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    # The sampler specifies how we should access the training data, which\n",
    "    # in this case is in a random order\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    \n",
    "    # TODO: Initailize the DataLoader (https://pytorch.org/docs/stable/data.html)\n",
    "    # so that it \n",
    "    # - loads from the provided train_dataset \n",
    "    # - samples using our sampler\n",
    "    # - uses the specified batch size\n",
    "    #\n",
    "    # NOTE: The batch size is pretty important! This says how many examples to train on \n",
    "    # at one time. If you recall, we talked about Stocastic Gradient Descent (SGD) that\n",
    "    # updates based on one instance at a time (e.g., changing the dog t-shirt size after seeing one dog)\n",
    "    # versus Gradient Descent (GD) that updates after all the data. SGD is much faster to\n",
    "    # converge to the \"right\" parameters but can make many missteps. The batch size \n",
    "    # says we can look at more than one instance at a time in determining how to update\n",
    "    # our parameters (e.g., look at a few dogs at a time to determine how to best update \n",
    "    # the t-shirt size, rather than just one dog or all the dogs)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    # How many total steps we'll take\n",
    "    t_total = len(train_dataloader) //  args.num_train_epochs\n",
    "\n",
    "    # The scheduler helps decide how quickly to update the weights based on how much\n",
    "    # training data we've seen. \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
    "    if os.path.exists(scheduler_last):\n",
    "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = args.start_step\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_acc = 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Note that this \"train_iterator\" is just tdqm wrapper that prints out which\n",
    "    # epoch we're currently in.     \n",
    "    train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    set_seed(args) \n",
    "    \n",
    "    # This tells pytorch that we're going to be changing the parameters so it needs\n",
    "    # to start keeping track of stuff\n",
    "    model.train()\n",
    "    for idx, _ in enumerate(train_iterator):\n",
    "        \n",
    "        # Keep train of the training loss (how \"bad\" the performance is) for this epohch\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        # For one epoch, loop over all the data, one batch at a time\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {'query_token_ids': batch[0],\n",
    "                      'code_token_ids': batch[1],\n",
    "                      'labels': batch[3]}\n",
    "            \n",
    "            ouputs = model(**inputs)\n",
    "            loss = ouputs[0]        \n",
    "            \n",
    "            # Do the back propagration to figure out which parameters to change.\n",
    "            # It's that easy!\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            # Update the parameters of our model based on the gradient and whatever\n",
    "            # else the optimizer is keeping track of\n",
    "            optimizer.step() \n",
    "            scheduler.step()  \n",
    "            \n",
    "            # This sets the gradient to zero before doing next update so we don't\n",
    "            # accidentally update the model based on the last batch's performance\n",
    "            model.zero_grad() \n",
    "            global_step += 1\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                break\n",
    "\n",
    "        # Once we finish an epoch, evaluate the model on the development data and see\n",
    "        # how well it does. We'll use this information to decide which version of\n",
    "        # the parameters to use.\n",
    "        results = evaluate(args, model, tokenizer, checkpoint=str(args.start_epoch + idx))\n",
    "\n",
    "        # \n",
    "        # Save the model and if we've already saved it, overwrite that saved model with \n",
    "        # the newly-trained parameters\n",
    "        #\n",
    "        last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "        if not os.path.exists(last_output_dir):\n",
    "            os.makedirs(last_output_dir)\n",
    "        model_to_save = model.module if hasattr(model,\n",
    "                                                'module') else model \n",
    "        model_to_save.save_pretrained(last_output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", last_output_dir)\n",
    "        idx_file = os.path.join(last_output_dir, 'idx_file.txt')\n",
    "        with open(idx_file, 'w', encoding='utf-8') as idxf:\n",
    "            idxf.write(str(args.start_epoch + idx) + '\\n')\n",
    "\n",
    "        torch.save(optimizer.state_dict(), os.path.join(last_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(last_output_dir, \"scheduler.pt\"))\n",
    "        logger.info(\"Saving optimizer and scheduler states to %s\", last_output_dir)\n",
    "\n",
    "        step_file = os.path.join(last_output_dir, 'step_file.txt')\n",
    "        with open(step_file, 'w', encoding='utf-8') as stepf:\n",
    "            stepf.write(str(global_step) + '\\n')\n",
    "\n",
    "        # Optional part 1 goes here\n",
    "\n",
    "        #\n",
    "        # If this model is better (on the training data) than the models from any of the \n",
    "        # past checkpoints, then keep a separate record of that too\n",
    "        #\n",
    "        if (results['acc'] > best_acc):\n",
    "            best_acc = results['acc']\n",
    "            output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            torch.save(args, os.path.join(output_dir, 'training_{}.bin'.format(idx)))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFAPZE8Fyxtk"
   },
   "source": [
    "# Set up the training environment\n",
    "\n",
    "This will get a few things ready for the model to train. You don't need to really do much in this block but it's worth seeing how it works if you want to train models in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1670371319402,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "gCXRZa9Ayxtl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 21:37:12 - INFO - __main__ -   reload model from ./models/checkpoint-last, resume from 3 epoch\n"
     ]
    }
   ],
   "source": [
    "# Setup CUDA so we can run on the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# This code will help us if we restart training and want to pick back up where we left off\n",
    "args.start_epoch = 0\n",
    "args.start_step = 0\n",
    "checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
    "    args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
    "    args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
    "    idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
    "    with open(idx_file, encoding='utf-8') as idxf:\n",
    "        args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
    "\n",
    "    step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
    "    if os.path.exists(step_file):\n",
    "        with open(step_file, encoding='utf-8') as stepf:\n",
    "            args.start_step = int(stepf.readlines()[0].strip())\n",
    "    logger.info(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIekkfvmyxtl"
   },
   "source": [
    "# Task: Setting up the bi-encoder model and its parameters\n",
    "\n",
    "### the TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372,
     "referenced_widgets": [
      "65a6b7d373664608b2624a16c9d83380",
      "512640cdd26443d9a980f77c192010d0",
      "ab410b27569743ce8ad5ff4acd261db5",
      "1e5bbe5bfb4449ab92edcbfc22571eda",
      "7b4995a9abfc490cb53576bde244a24e",
      "8686da7f60244bfa8de75ec5f70b26bf",
      "c0082bdb0da74c99bff20110d2f6ed3f",
      "d02158f8d80340b39f7f2c3dbf7d5599",
      "b655c0357a3541f0b78ac2f2fe2ebf27",
      "545574aa878f4fe0a38e8ba82f3090ab",
      "b6b456a89f1a4328b534e68311eafaea",
      "c738f960d8a540b295a85853d6a3b78f",
      "81381720e36c446085c8a69b79780425",
      "275fd704c2d44feba32c75424ba13b7d",
      "0acdf0b867cf464dba6d4b18d334e0b9",
      "793bf706fc36491c80ff960d4578f8da",
      "9702b553eabc4de385ac63b5ed340160",
      "5af203aa80a14560a69dde3dd99ef665",
      "e0cc18ea54f846f5963b305aa53005e8",
      "ac0d62afbe1944bfa83b3ba6a9494f79",
      "079031510c1b4b3897c3026ef8dad9a4",
      "49a1c5c637594df5be80b9480a94d4e6",
      "3990eed327e345baab462961fc6aa300",
      "b86cc6b4cb46455db86a1264a8d1a13d",
      "e89999b1ee3c44358abd66d90297d698",
      "5749880ed5c84d4ca82e0570b6a20fac",
      "6ddb7145af3344368239426589e71b4e",
      "9c344891ac0147f6ac8bef5a94961efb",
      "0373b11ff4dc480183e1467dfb44541e",
      "1f67c0c70cdf420b8e0d926f013fbea5",
      "3ea5523615eb471b83aa4ecd96354e54",
      "3faf2597fe6148e39c8595e9084a84bd",
      "40fb2b354fd5411db8fa8dafec540c65",
      "428f47d082d548c794c3877be8f92211",
      "eb64f05bc8ee4e1faff7b03e726edff5",
      "0069b09b1f1b4decab94c40dfa65f83f",
      "bc0e6ab15cdd4f519d5578af6b7156c9",
      "f9ed006a5da3476196ad93c51ad09aee",
      "314614ea95014ec1b45809b2723e9a34",
      "bb7298e53350462fba7054d97483c2f7",
      "1942718b36684b43af3fcab7b6e26851",
      "b79aec44f5c8458097e7073dc3506ff2",
      "6f88a17b4c8b4280ae0503b29028290a",
      "9342bd3e0e8a4005bd181c48e74a79b3",
      "40c57f544d894d67a9a672ba40eff578",
      "2f77cc2ef3954a90a39a6c313c70db20",
      "8b26e0836aa9477a98e186b173c9a103",
      "969607d9b3de49beb77b1f8c0a6a633f",
      "80f94782f5894afd90f48d32d8577b2e",
      "93fe0b9515b94a79ba8ac060a57f0edc",
      "8a7783c1125c46ae9e6971449aac7ebe",
      "1585fa06a3484511b26522604eb8a26b",
      "47380b37dac844f9b1d443efe0e668f5",
      "1a719550d8284eee876dfcdef2b2374e",
      "59b1b791c64247b785355144c183fa09",
      "1fec94f05199469a83ce811f2b75020d",
      "22eae3c5ac864298838da083330c32a4",
      "c8827707070347eca302cca686a70dc8",
      "a8de86d9d8804ddaa05d300b58514dfb",
      "69e57f49b7c74fa3a12de04e779b1b90",
      "e2c4dd2a706d42e69a34a5fa3042de89",
      "87a8a07f75194143b0983e6a9daf226b",
      "55a5a37999c44ac3bfc7c3251c1ca5f7",
      "181cf860cf834ab9a60ab2806f171933",
      "249dfe5cb9404ec6a70a476e44d57b55",
      "75a35c1e924d4857ba36092aeb3a51de"
     ]
    },
    "executionInfo": {
     "elapsed": 35073,
     "status": "ok",
     "timestamp": 1670371382971,
     "user": {
      "displayName": "Yi-An Chu",
      "userId": "13218931171162323923"
     },
     "user_tz": 300
    },
    "id": "duQvPY8iyxtm",
    "outputId": "bb88093a-7c01-423d-c666-c3260e0ac01f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/yianchu/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We'll specify some general configurations that tell the models what kind\n",
    "# of parameters to use and how to turn incoming text/code data into identifiers for \n",
    "# processing with the neural network \n",
    "#\n",
    "# We set num_labels = 1 because this is a regression class\n",
    "# (compared to a classification task with many class labels)\n",
    "num_labels = 1\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base',\n",
    "                                      num_labels=num_labels, finetuning_task=args.task_name)\n",
    "# We'll treat relevance as a regression problem\n",
    "config.problem_type = 'regression'\n",
    "\n",
    "# If you remember from our neural language model part of the lecture, we talked\n",
    "# about one language model that gets fed a series of words to predict the next\n",
    "# and each word is mapped to an embedding. The \"tokenizer\" specifies how to \n",
    "# break up words into tokens but it frequently doesn't use just spaces!\n",
    "# In fact, most tokenizers break words into *pieces* to reduce the size of the\n",
    "# vocabularly (fewer embeddings!) so we need to specify which tokenizer to use.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "# Now comes the time to define our model. Let's specify the model class (which we'll need later)\n",
    "model_class = CodeSearchBiencoderModel\n",
    "# And we'll instantiate the model itself.\n",
    "model = CodeSearchBiencoderModel(config)\n",
    "\n",
    "# Now comes the magic where we specify the two encoders. Conveniently for us,\n",
    "# there's actually a very recent langauge model that knows *both* code and human language!!\n",
    "# We'll use this set of parameters to initialize *each* of our encoders. Over time,\n",
    "# each encoder's parameters will start to become different since one side is going\n",
    "# to learn how to encode queries better and the other will learn how to encode \n",
    "# code documents.\n",
    "#\n",
    "# NOTE: There's nothing stopping us from trying other parameters for the\n",
    "# encoders too. If you're feeling curious you could swap in any RoBERTa model\n",
    "# for the query encoder and it will just work.\n",
    "#\n",
    "# TODO: Initialize each of the coders using the \"from_pretrained\" method and\n",
    "# specifying the pretrained model you want. Here, we'll use the CodeBERT model, \n",
    "# which is hosted on Huggingface https://huggingface.co/microsoft/codebert-base\n",
    "# You should pass in the full name of the pretrained model (which includes the \"/\").\n",
    "# Note that this code is going to look the same for both encoders and may \n",
    "# seem kind of easy to do but we want you to see how to do it yourself. :) \n",
    "model.query_encoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.code_encoder = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# This will move the model's parameters onto the GPU so it runs fast\n",
    "model.to(args.device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "# Remember how we talked about stochastic gradient descent (SGD)? Well, it's not\n",
    "# the only way to update parameters. There are many (many) ways to do this\n",
    "# and the usual standard is actually AdamW which uses a bit of bookkeeping to figure\n",
    "# out how to update the weights more efficiently so the model learns faster.\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "# If we're restarting, load the optimizer's state at the last time step\n",
    "optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
    "if os.path.exists(optimizer_last):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZeN-HA6yxtp"
   },
   "source": [
    "# Do the training!\n",
    "\n",
    "Finally!! Let's train that model and save it to a file so we can evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXhcHk0ayxtq",
    "outputId": "9290f1cb-b244-4b06-cc18-c9c325a3ccab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 21:37:28 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x1502c5bbb2e0>\n",
      "12/06/2022 21:37:28 - INFO - run_classifier -   Loading features from cached file ./data/cached_train_train_300k_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 21:37:28 - INFO - run_classifier -   Creating features from dataset file at ./data\n",
      "12/06/2022 21:37:28 - INFO - utils -   LOOKING AT ./data/train_300k.txt\n",
      "12/06/2022 21:37:30 - INFO - utils -   Writing example 0 of 247302\n",
      "12/06/2022 21:37:30 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:37:30 - INFO - utils -   guid: train-0\n",
      "12/06/2022 21:37:30 - INFO - utils -   query_token_ids: 0 18377 10 762 8 10 14792 28700 25 5129 30 36410 7954 36418 5368 41 3901 4327 9 42766 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   code_token_ids: 0 9232 13075 7954 36 3741 29 2156 45655 2156 14792 2156 3000 2156 762 4839 4832 671 3741 29 36 45655 2156 3000 2156 762 2156 1009 1009 14792 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   label: 1 (id = 1)\n",
      "12/06/2022 21:37:30 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:37:30 - INFO - utils -   guid: train-1\n",
      "12/06/2022 21:37:30 - INFO - utils -   query_token_ids: 0 47596 1178 479 3023 47596 479 1316 479 2189 479 120 46650 43292 47897 12736 18400 45262 25448 36484 20024 18164 47876 18400 36484 20024 7258 47645 5543 36484 2840 7487 48944 15722 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   code_token_ids: 0 9232 2394 1215 6460 36 1403 2156 32715 4839 4832 2069 5457 17918 45589 36 128 47596 1178 4 47596 1178 4 36617 4 30821 4 6460 108 4839 2069 646 128 16346 108 27779 5457 32715 1403 479 1045 36 1403 479 11189 36 2069 4839 4839 671 1403 479 898 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:37:30 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:37:30 - INFO - utils -   guid: train-2\n",
      "12/06/2022 21:37:30 - INFO - utils -   query_token_ids: 0 347 25522 13516 16093 34222 35524 16 25 230 25522 479 43756 35524 230 25522 1300 14699 35524 34 5 276 46195 3099 25 230 25522 479 43756 35524 53 122 67 14564 13516 16093 8720 479 230 25522 326 329 23999 35524 14564 10 326 329 23999 2\n",
      "12/06/2022 21:37:30 - INFO - utils -   code_token_ids: 0 9232 43756 23858 36 1403 2156 13516 16093 34222 2156 1300 14699 5457 9291 2156 326 329 23999 5457 9291 2156 1732 5457 9291 4839 4832 849 114 1300 14699 34 10 44346 45926 5448 6 304 5 90 6 1493 6 95 1323 5 849 1445 631 7 43756 8 18644 5 3018 2215 99 5 7105 51 32 849 608 4 1300 14699 5457 120 44156 36 1300 14699 2156 128 10519 594 45926 108 2156 36 49027 4832 1300 14699 4839 4839 36 4839 849 370 42608 41814 28 634 19290 12527 4 1437 8630 400 2072 114 577 6 849 11597 114 45 4 1437 6068 6 9291 16 10 8218 326 329 23999 7626 11 5 403 9 849 5 11355 14157 4 400 2072 5457 120 44156 36 326 329 23999 2156 128 18076 2072 108 2156 36 49027 385 90 4832 2\n",
      "12/06/2022 21:37:30 - INFO - utils -   label: 1 (id = 1)\n",
      "12/06/2022 21:37:30 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:37:30 - INFO - utils -   guid: train-3\n",
      "12/06/2022 21:37:30 - INFO - utils -   query_token_ids: 0 9157 24038 24047 35354 7 10 19790 295 417 30766 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   code_token_ids: 0 9232 455 36 1403 4839 4832 849 10081 4467 7281 368 25451 552 78 5745 9515 5457 19974 479 16007 36 1403 479 2433 646 321 27779 2156 449 8849 1069 1215 763 139 36 1403 479 2433 646 112 4832 27779 4839 479 255 4839 849 96 15189 25451 552 78 5745 671 19974 479 14098 5776 36 9515 2156 1403 479 3989 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   label: 1 (id = 1)\n",
      "12/06/2022 21:37:30 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:37:30 - INFO - utils -   guid: train-4\n",
      "12/06/2022 21:37:30 - INFO - utils -   query_token_ids: 0 34256 8267 13 1765 4832 26437 4832 4615 1215 28615 19 42 6029 11 5 78 4795 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   code_token_ids: 0 9232 4615 1215 28615 36 1403 2156 14929 4832 3310 868 646 646 11373 27779 2156 49460 27779 5457 9291 2156 1009 2156 19220 1215 48702 1215 10799 4832 6979 5457 321 4839 4832 671 4615 1215 28615 36 646 1403 27779 2156 14929 2156 19220 1215 48702 1215 10799 5457 19220 1215 48702 1215 10799 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:37:30 - INFO - utils -   label: 1 (id = 1)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "12/06/2022 21:37:36 - INFO - utils -   Writing example 10000 of 247302\n",
      "12/06/2022 21:37:43 - INFO - utils -   Writing example 20000 of 247302\n",
      "12/06/2022 21:37:50 - INFO - utils -   Writing example 30000 of 247302\n",
      "12/06/2022 21:37:57 - INFO - utils -   Writing example 40000 of 247302\n",
      "12/06/2022 21:38:04 - INFO - utils -   Writing example 50000 of 247302\n",
      "12/06/2022 21:38:11 - INFO - utils -   Writing example 60000 of 247302\n",
      "12/06/2022 21:38:18 - INFO - utils -   Writing example 70000 of 247302\n",
      "12/06/2022 21:38:25 - INFO - utils -   Writing example 80000 of 247302\n",
      "12/06/2022 21:38:32 - INFO - utils -   Writing example 90000 of 247302\n",
      "12/06/2022 21:38:38 - INFO - utils -   Writing example 100000 of 247302\n",
      "12/06/2022 21:38:46 - INFO - utils -   Writing example 110000 of 247302\n",
      "12/06/2022 21:38:53 - INFO - utils -   Writing example 120000 of 247302\n",
      "12/06/2022 21:39:00 - INFO - utils -   Writing example 130000 of 247302\n",
      "12/06/2022 21:39:06 - INFO - utils -   Writing example 140000 of 247302\n",
      "12/06/2022 21:39:13 - INFO - utils -   Writing example 150000 of 247302\n",
      "12/06/2022 21:39:20 - INFO - utils -   Writing example 160000 of 247302\n",
      "12/06/2022 21:39:28 - INFO - utils -   Writing example 170000 of 247302\n",
      "12/06/2022 21:39:35 - INFO - utils -   Writing example 180000 of 247302\n",
      "12/06/2022 21:39:42 - INFO - utils -   Writing example 190000 of 247302\n",
      "12/06/2022 21:39:49 - INFO - utils -   Writing example 200000 of 247302\n",
      "12/06/2022 21:39:55 - INFO - utils -   Writing example 210000 of 247302\n",
      "12/06/2022 21:40:03 - INFO - utils -   Writing example 220000 of 247302\n",
      "12/06/2022 21:40:10 - INFO - utils -   Writing example 230000 of 247302\n",
      "12/06/2022 21:40:17 - INFO - utils -   Writing example 240000 of 247302\n",
      "12/06/2022 21:40:22 - INFO - run_classifier -   Saving features into cached file ./data/cached_train_train_300k_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 21:40:43 - INFO - __main__ -   ***** Running training *****\n",
      "12/06/2022 21:40:43 - INFO - __main__ -     Num examples = 247302\n",
      "12/06/2022 21:40:43 - INFO - __main__ -     Num Epochs = 3\n",
      "12/06/2022 21:40:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "12/06/2022 21:40:43 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/06/2022 21:40:43 - INFO - __main__ -     Total optimization steps = 1288\n",
      "Epoch: 0it [00:00, ?it/s]\n",
      "12/06/2022 21:40:43 - INFO - __main__ -    global_step = 3, average loss = 0.0\n",
      "12/06/2022 21:40:43 - INFO - __main__ -   Saving model checkpoint to ./models\n",
      "Some weights of the model checkpoint at ./models were not used when initializing RobertaModel: ['query_encoder.encoder.layer.7.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.10.output.dense.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.output.dense.weight', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'query_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.10.output.dense.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.8.output.dense.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'query_encoder.embeddings.position_embeddings.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'query_encoder.encoder.layer.9.output.dense.bias', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'query_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models and are newly initialized: ['encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Load in the training dataset. Here, we've handled most of the data preprocessing for you\n",
    "train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, ttype='train')\n",
    "\n",
    "# Call the training function that we defined above\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer, optimizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# Save the trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(args.output_dir)\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = AutoModel.from_pretrained(args.output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOcgn6Qayxtq"
   },
   "source": [
    "# Evaluate the best model on the test data\n",
    "\n",
    "Our code in training keeps track of how the model is doing and currently keeps around the files for the \"best performing\" model on the training data. How well does this model do on the test data? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wXYpj585yxtr",
    "outputId": "2651d85a-dc2f-42b7-e347-802622dcc70d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 21:40:54 - INFO - __main__ -   Evaluate the following checkpoint: ./models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 21:40:56 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 21:40:56 - INFO - run_classifier -   Creating features from dataset file at ./data\n",
      "12/06/2022 21:40:56 - INFO - utils -   LOOKING AT ./data/valid.txt\n",
      "12/06/2022 21:40:57 - INFO - utils -   Writing example 0 of 46213\n",
      "12/06/2022 21:40:57 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:40:57 - INFO - utils -   guid: dev-0\n",
      "12/06/2022 21:40:57 - INFO - utils -   query_token_ids: 0 6407 13851 37357 5 2324 828 2622 30 5222 41 3169 19470 463 36 192 20387 39891 463 37380 1215 347 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   code_token_ids: 0 9232 386 1215 9981 10845 36 1403 2156 2345 2156 1100 2156 425 5457 9291 2156 414 5457 9291 2156 17017 5457 9291 2156 923 5457 321 2156 1123 5457 883 612 4839 4832 18088 1403 479 18134 642 4345 1215 9981 10845 16 9291 2156 22 44307 554 48726 113 1403 479 18134 642 4345 1215 9981 10845 5457 221 4345 48132 36 2345 2156 1100 2156 425 2156 414 2156 17017 2156 923 2156 1123 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:40:57 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:40:57 - INFO - utils -   guid: dev-1\n",
      "12/06/2022 21:40:57 - INFO - utils -   query_token_ids: 0 47167 70 5 6773 11 10 576 31826 18099 129 6773 19 5 576 5064 114 17966 479 20 576 449 605 48204 32 1595 149 7 5 2340 47073 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   code_token_ids: 0 9232 3116 1215 12376 36 7425 1215 22609 2156 414 2156 1157 1215 48696 5457 7447 4839 4832 40462 5457 11988 479 2718 479 16 21710 36 7425 1215 22609 4839 16 8458 5457 16 48768 36 414 2156 889 4839 114 40462 8 45 1157 1215 48696 4832 1693 47617 36 128 19186 4345 34 57 6242 108 128 8 2870 7606 29 8785 108 7606 7425 1215 22609 4839 114 45 36 16 8458 50 16 48768 36 414 2156 1586 15453 4839 4839 4832 1693 47617 36 128 10836 129 3116 1586 15453 8720 50 36451 108 128 8204 7 7425 2870 955 4839 21634 5457 414 114 16 8458 1493 414 479 21634 114 45 70 36 16 48768 36 1615 2156 28700 4839 13 1615 11 21634 4839 4832 1693 47617 36 128 36583 4785 531 28 45073 5119 955 4839 7425 1215 2\n",
      "12/06/2022 21:40:57 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:40:57 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:40:57 - INFO - utils -   guid: dev-2\n",
      "12/06/2022 21:40:57 - INFO - utils -   query_token_ids: 0 22376 7022 819 9 9188 46249 47184 36 27290 4839 634 5 39825 740 47555 20686 8 2740 30 22367 86 192 141 7 422 15 5936 111 516 1065 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   code_token_ids: 0 9232 4392 14386 36 326 119 21527 2156 326 119 45137 2156 295 33177 29 4839 4832 849 1045 27290 4327 7 2450 326 119 5457 326 119 21527 36 346 10643 18551 29 5457 326 119 45137 4839 849 5368 8135 414 414 5457 295 35187 479 9624 479 20979 2544 36 321 2156 132 2156 646 326 119 45137 2156 295 33177 29 27779 4839 479 12976 37356 36 128 46349 2881 108 4839 13 939 11 3023 9435 36 295 33177 29 4839 4832 849 92 414 358 86 6 42 16 5 2373 403 819 849 588 819 74 28 357 6 25 5 8135 414 74 45 28 2198 9624 385 5457 414 646 4832 2156 939 27779 849 5 3031 5043 7 4392 328 326 119 479 37357 36 385 2156 7447 4839 2 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   label: 1 (id = 1)\n",
      "12/06/2022 21:40:57 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:40:57 - INFO - utils -   guid: dev-3\n",
      "12/06/2022 21:40:57 - INFO - utils -   query_token_ids: 0 15953 368 2630 7 769 111 860 21013 1519 71 1996 5 3018 13 24790 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   code_token_ids: 0 9232 18134 26628 44298 36 48930 4839 4832 787 1531 3894 22890 479 18166 36 48930 4839 3816 8144 36 1009 49503 2156 1009 1009 449 605 48204 4839 4832 849 48639 6 2156 36106 14263 1434 6 3425 31 1423 16037 479 21748 6595 48639 849 114 1423 16037 16 145 422 786 8007 21574 6 172 52 393 5494 1506 6 53 52 849 109 486 48639 4 11515 2072 44518 6 98 14 10 27754 33000 64 28 7899 35 10813 5457 720 17075 479 120 36 128 8007 12228 108 4839 860 4832 671 48930 36 1009 49503 2156 1009 1009 449 605 48204 4839 4682 5034 479 18286 479 44454 30192 25 364 4832 114 364 479 1263 479 2194 1215 20414 45994 5034 479 14284 479 18893 4832 849 642 4360 2544 35 33022 5214 2362 12 8648 37764 479 47423 36 2\n",
      "12/06/2022 21:40:57 - INFO - utils -   label: 1 (id = 1)\n",
      "12/06/2022 21:40:57 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:40:57 - INFO - utils -   guid: dev-4\n",
      "12/06/2022 21:40:57 - INFO - utils -   query_token_ids: 0 20763 19099 36 321 479 321 111 112 479 321 4839 3543 227 32833 2351 106 114 2139 479 20 2408 3372 5 3585 9 5 2748 36 45 5 701 4839 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   code_token_ids: 0 9232 3438 1215 46840 36 1403 2156 13561 4839 4832 114 1403 479 34 1215 5282 36 13561 4839 4832 295 5457 1403 646 13561 27779 1403 479 32833 479 3438 36 295 4839 2424 1403 646 13561 27779 849 27336 70 15716 3329 13561 8 70 5678 7 24 4 13 364 11 889 36 1403 479 15716 4839 4832 114 295 11 36 364 479 37908 134 2156 364 479 37908 176 4839 4832 114 295 11 364 479 37908 134 479 5678 4832 364 479 37908 134 479 5678 479 3438 36 295 4839 114 295 11 364 479 37908 176 479 5678 4832 364 479 37908 176 479 5678 479 3438 36 295 4839 1403 479 15716 479 3438 36 364 4839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:40:57 - INFO - utils -   label: 0 (id = 0)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "12/06/2022 21:41:04 - INFO - utils -   Writing example 10000 of 46213\n",
      "12/06/2022 21:41:11 - INFO - utils -   Writing example 20000 of 46213\n",
      "12/06/2022 21:41:18 - INFO - utils -   Writing example 30000 of 46213\n",
      "12/06/2022 21:41:25 - INFO - utils -   Writing example 40000 of 46213\n",
      "12/06/2022 21:41:29 - INFO - run_classifier -   Saving features into cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 21:41:33 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 21:41:33 - INFO - run_classifier -     Num examples = 46213\n",
      "12/06/2022 21:41:33 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [02:31<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5000108194663839, 'f1_': 0.6666762839007502, 'acc_and_f1_': 0.583343551683567}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "checkpoint = args.output_dir\n",
    "\n",
    "logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
    "\n",
    "print(checkpoint)\n",
    "global_step = \"\"\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "result = evaluate(args, model, tokenizer, checkpoint=checkpoint, prefix=global_step)\n",
    "result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "print(result)\n",
    "\n",
    "    \n",
    "# Optional part 3 goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qixrKejmJ-N5"
   },
   "source": [
    "# Doing Inference on the Test Dataset\n",
    "\n",
    "Finally, let's estimate the relevance scores for the query-document pairs in our test dataset. The test dataset **test_data.csv** contains each pair of 99 queries and 958 documents, which in total adds up to 94,842 query-document annotations (compare that with the project update number!). \n",
    "\n",
    "For ease of this exercise, we've already processed the data into a ready-to-go format in **test_data.txt** which is required by the model. In the following block, for each query-document pair, we generate a prediction score that measure the relevance of that pair by feeding the pair as inputs to the model's `forward` function (note that in pytorch if you have some model, doing `model(inputs)` and `model.forward(inputs)` is the same--it's trying to emphasize thinking of these as functions!). \n",
    "\n",
    "Once we have the model predictions, we'll create a new dataframe that contains (1) the query id, (2) the document id, and (3) the relevance score for that pair. We'll hand this dataframe off to Part 2 so you can finish up your GPU work.\n",
    "\n",
    "You should adjust the directory path accordingly in order to successfully do the inference. Check what you got in the result.txt file and write this back into the **test_data.csv** that adds an additional column \"sim\". Later In Part 2, you will incorporate the prediction score into the learning to rank model to see if it can improve the performance of ranking,\n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "This implementation works because we've aligned the `test_data.txt` and `test_data.csv` files so they're in the same order. That means you can write a long list of similarities and then add it back to the test data's DataFrame and it will Just Workâ„¢. However, in production settings, it's often useful to keep identifiers with the data as much as possible so that you don't just have a file of predictions and instead can write the predictions with the query/document identifiers (or whatever data you're working with).\n",
    "\n",
    "In this homework's setup, we're precomputed the relevance scores for later integration with some overall ranking function (done in Part 2). To get these, we re-encode everything for each step. In commercial systems, what is typically done is the documents are encoded once and then cached, much like how our inverted index caches the terms in each document. Then when a new queries arrives, we only have to encode it and compare it with the cached document emeddings. This saves a lot of time! Thankfully our dataset is quite small here so we don't need to do that, but the ability to precompute and cache embeddings is worth remembering why bi-encoders are helpful and efficient for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ymWArT6Wyxts",
    "outputId": "8afcc7b4-bc78-4e59-d18d-58f0c00db172",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 21:44:04 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 21:44:04 - INFO - run_classifier -   Creating features from dataset file at ./data\n",
      "12/06/2022 21:44:04 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "12/06/2022 21:44:04 - INFO - utils -   Writing example 0 of 94842\n",
      "12/06/2022 21:44:04 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:44:04 - INFO - utils -   guid: test-0\n",
      "12/06/2022 21:44:04 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   code_token_ids: 0 9232 3116 1215 49079 1640 49451 6 414 6 45382 8660 5214 6842 846 1215 495 3721 3755 2068 2076 3256 49434 21062 2274 414 7 47896 2870 4832 46669 48786 35 766 9 47896 2870 7 3116 414 7 4832 12528 48786 35 7031 4832 46669 414 35 2274 414 7 3116 7 47896 2870 4832 12528 414 35 295 35187 8932 4832 46669 45382 8660 35 45382 8660 341 11 47896 2870 4 35364 16 45518 131 49519 4832 12528 45382 8660 35 7031 49434 19 490 1640 49451 6 128 605 27645 25 2870 35 740 36245 1215 9408 5457 740 36245 4 9408 1640 21710 6 45382 8660 5214 10273 757 8660 43 13 516 11 414 35 740 36245 1215 9408 4 9408 1722 1640 1902 43 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:44:04 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:44:04 - INFO - utils -   guid: test-1\n",
      "12/06/2022 21:44:04 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   code_token_ids: 0 9232 3116 1215 560 1215 49079 1640 506 13650 6 12734 6 22162 3256 19 490 1640 506 13650 6 128 41161 27645 25 740 36245 21710 35 740 36245 9408 5457 740 36245 4 9408 1640 49079 21710 6 45382 8660 5214 3934 3934 9740 24262 47579 15483 3934 17977 5214 49079 4 15513 28002 1215 24765 3755 2118 43 740 36245 9408 4 9408 1722 1640 24419 43 13 3236 11 22162 35 740 36245 9408 4 9408 1722 1640 646 29 4 225 20414 46469 49118 12 398 18653 44654 1640 3934 3934 12801 322 44654 45803 37457 282 3934 12801 43 13 579 11 3236 45587 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:44:04 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:44:04 - INFO - utils -   guid: test-2\n",
      "12/06/2022 21:44:04 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   code_token_ids: 0 9232 3116 1215 49079 1640 13367 3256 849 2315 78 7510 18 2870 2718 98 52 3116 47896 7 276 31826 25 84 414 740 36245 1215 22609 5457 1403 48030 22609 73 108 16008 22597 4 49079 108 19 490 1640 49079 1215 22609 6 128 605 27645 25 856 35 740 36245 1215 9408 5457 740 36245 4 9408 1640 506 43 740 36245 1215 9408 4 9408 1722 1640 48759 13650 47429 33480 108 45587 13 1763 11 1403 48030 49079 1215 25867 4 46740 49536 1763 5457 646 366 4 22609 4 5982 22609 1640 44170 10975 288 7479 1403 48030 22609 238 1763 10975 134 48392 740 36245 1215 9408 4 9408 1722 1640 44170 43 671 740 36245 1215 22609 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:44:04 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:44:04 - INFO - utils -   guid: test-3\n",
      "12/06/2022 21:44:04 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   code_token_ids: 0 9232 7 1215 49079 1640 13367 6 856 48939 6 9740 24262 47579 113 3934 45382 8660 5214 47429 3256 740 36245 9408 5457 740 36245 4 9408 1640 856 48939 6 9740 24262 5214 43948 24262 6 45382 8660 5214 10273 757 8660 6 24248 39938 2630 40635 37457 282 1297 17977 5214 49079 4 15513 28002 1215 7981 4839 114 1403 4 47288 35 740 36245 9408 4 9408 1722 1640 13367 4 47288 43 13 516 11 1403 4 8628 462 330 1215 45291 4 8656 1215 13415 49536 740 36245 9408 4 9408 1722 1640 1902 43 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:44:04 - INFO - utils -   *** Example ***\n",
      "12/06/2022 21:44:04 - INFO - utils -   guid: test-4\n",
      "12/06/2022 21:44:04 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/06/2022 21:44:04 - INFO - utils -   code_token_ids: 0 9232 3116 1215 560 1215 49079 1640 13367 6 48786 3256 128 17809 3015 8303 7 2007 740 36245 4832 46669 7031 48786 35 16234 7 2870 13 5376 128 17809 31243 5457 490 1640 49451 6 128 43192 27645 849 21384 12734 889 12734 1215 23999 5457 47052 21001 11964 3934 128 41933 11964 3934 128 45566 3934 128 40089 13539 12440 3934 128 23996 9210 4183 14064 3934 128 428 12 19434 44403 3331 5457 740 36245 4 495 11726 45489 1640 506 808 6 882 37815 5214 24419 1215 23999 43 42211 5457 28700 48461 13650 288 6 766 288 43 13 766 288 11 12734 1215 23999 43 849 21062 7 2870 3331 4 9408 1722 1640 47288 43 13 3236 11 1403 4 23687 35 849 14619 2976 26640 30 1021 37740 332 19 117 38686 24414 849 8 602 2093 9 5 2\n",
      "12/06/2022 21:44:04 - INFO - utils -   label: 0 (id = 0)\n",
      "12/06/2022 21:44:11 - INFO - utils -   Writing example 10000 of 94842\n",
      "12/06/2022 21:44:17 - INFO - utils -   Writing example 20000 of 94842\n",
      "12/06/2022 21:44:23 - INFO - utils -   Writing example 30000 of 94842\n",
      "12/06/2022 21:44:29 - INFO - utils -   Writing example 40000 of 94842\n",
      "12/06/2022 21:44:35 - INFO - utils -   Writing example 50000 of 94842\n",
      "12/06/2022 21:44:42 - INFO - utils -   Writing example 60000 of 94842\n",
      "12/06/2022 21:44:48 - INFO - utils -   Writing example 70000 of 94842\n",
      "12/06/2022 21:44:54 - INFO - utils -   Writing example 80000 of 94842\n",
      "12/06/2022 21:45:00 - INFO - utils -   Writing example 90000 of 94842\n",
      "12/06/2022 21:45:03 - INFO - run_classifier -   Saving features into cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [05:08<00:00,  4.80it/s]\n",
      "12/06/2022 21:50:20 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    }
   ],
   "source": [
    "# This tells the model that we're switching to evaluation mode (rather than training)\n",
    "# so it should turn off any training-specific functionality that could make it slower\n",
    "# or interfere with our results.\n",
    "model.eval()\n",
    "    \n",
    "# Note here: we're loading the test set ***in sequential order***. This is critical\n",
    "# for the next step because we need to map these predictions to query-document pairs.\n",
    "# In training, we want to see a random order, but typically not during test.\n",
    "eval_dataset, instances = load_and_cache_examples(args, \"codesearch\", tokenizer, ttype='test')\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "# This data structures will have our predictions and we'll fill them as we process each batch\n",
    "relevance_predictions = np.array([])\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "    # Get the model's cosine similarity for the query-document pairs\n",
    "    # we pass as input. This no_grad() call also tells pytorch that\n",
    "    # we're doing evaluation so pytorch doesn't have to keep track\n",
    "    # of any gradients for updating the model (e.g., remember how\n",
    "    # in the dog t-shirt fitting, we had to keep around how much to change\n",
    "    # the t-shirt sizes).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Prepare the inputs\n",
    "        inputs = {'query_token_ids': batch[0],\n",
    "                  'code_token_ids': batch[1],\n",
    "                  'labels': batch[3]}\n",
    "       \n",
    "        # Note that this is a list of outputs, which includes the cosine\n",
    "        # similarity, among other stuff\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Let's pull out just the cosine similarity\n",
    "    _, cosine_sim = outputs[:2] \n",
    "    \n",
    "    # Pytorch works with \"tensors\" which are just like fancy numpy arrays.\n",
    "    # One main difference is that the tensor might \"live\" on a GPU, which \n",
    "    # means we need to copy it into regular computer memory to use it.\n",
    "    # Here, we'll call .cpu() to get the value back off the GPU and then\n",
    "    # convert the similarities to numpy. Remember, we're getting a list\n",
    "    # of similarities back out!\n",
    "    cosine_sims = cosine_sim.cpu().numpy()\n",
    "    \n",
    "    # Add these similarities to our current similarities\n",
    "    relevance_predictions = np.append(relevance_predictions, cosine_sims, axis=0)\n",
    "\n",
    "if not os.path.exists(args.test_result_dir):\n",
    "    os.makedirs(args.test_result_dir)\n",
    "\n",
    "output_test_file = os.path.join(args.test_result_dir, 'relevance-scores.csv')\n",
    "\n",
    "with open(output_test_file, \"w\") as outf:\n",
    "    logger.info(\"***** Writing relevance predictions *****\")\n",
    "    all_logits = relevance_predictions.tolist()\n",
    "    \n",
    "    # Note that we write these all as one big list. In the next step,\n",
    "    # we'll merge these with the data frame\n",
    "    outf.write(\",\".join([str(item) for item in all_logits]))\n",
    "    \n",
    "# Optional part 3 goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocZw28zLyxtt"
   },
   "source": [
    "### TODO: Merge the predictions with the query/doc pairs (10 points)\n",
    "\n",
    "The numpy array `relevance_predictions` now contains a list of all the similarities, which we'll need to merge with the test data. Conveniently, these predictions appear in the exact same order as the query-document pairs in the `data/test_data.csv` file.\n",
    "\n",
    "Your task is to read in `test_data.csv` as a dataframe and merge these relevance predictions as a new column called \"sim\". We'll export this dataframe to a separate file with just a few columns for better efficiency. Write a _new_ file with a subset of this dataframe containing only the columns\n",
    "* \"sim\"\n",
    "* \"qid\" (the query id)\n",
    "* \"docno\" (the document id)\n",
    "These last two columns match the pyterrier naming conventions, which you'll need for Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NHFXPEAgyxtt",
    "outputId": "568792b7-1831-48ab-f8e9-46c3cdff1858"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Query</th>\n",
       "      <th>GitHubUrl</th>\n",
       "      <th>code</th>\n",
       "      <th>Query_id</th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>relevance</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/sentinel-hub/sentinelhub-py...</td>\n",
       "      <td>def write_csv(filename, data, delimiter=CSV_DE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/jordanjoz1/flickr-views-cou...</td>\n",
       "      <td>def write_to_csv(fname, header, rows): with op...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/fastai/fastai/blob/9fb84a5c...</td>\n",
       "      <td>def write_csv(self): # Get first element's fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/majerteam/sylk_parser/blob/...</td>\n",
       "      <td>def to_csv(self, fbuf, quotechar='\"', delimite...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/gem/oq-engine/blob/8294553a...</td>\n",
       "      <td>def write_to_csv(self, filename): ''' Exports ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Query                                          GitHubUrl  \\\n",
       "0           0  write csv  https://github.com/sentinel-hub/sentinelhub-py...   \n",
       "1           1  write csv  https://github.com/jordanjoz1/flickr-views-cou...   \n",
       "2           2  write csv  https://github.com/fastai/fastai/blob/9fb84a5c...   \n",
       "3           3  write csv  https://github.com/majerteam/sylk_parser/blob/...   \n",
       "4           4  write csv  https://github.com/gem/oq-engine/blob/8294553a...   \n",
       "\n",
       "                                                code  Query_id  Doc_id  \\\n",
       "0  def write_csv(filename, data, delimiter=CSV_DE...         0       0   \n",
       "1  def write_to_csv(fname, header, rows): with op...         0       1   \n",
       "2  def write_csv(self): # Get first element's fil...         0       2   \n",
       "3  def to_csv(self, fbuf, quotechar='\"', delimite...         0       3   \n",
       "4  def write_to_csv(self, filename): ''' Exports ...         0       4   \n",
       "\n",
       "   relevance Language  \n",
       "0          3   Python  \n",
       "1          3   Python  \n",
       "2          3   Python  \n",
       "3          3   Python  \n",
       "4          3   Python  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/test_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQEEFP3hyxtu"
   },
   "source": [
    "# _Optional TODO_: Evaluating the different models (20 points total; this is part 1)\n",
    "\n",
    " In the code above, we save the model's parameter for the most recent epoch and an extra directory for saving the model with the highest accuracy so far on the validation data. How much training does the model actually need to recognize relevance? Would one epoch be enough? What if we did 10? or 100? (100 might be too many for Great Lakes limits...). In this **optional part**, we'll describe a series of steps you can take to explore this part!\n",
    " \n",
    "Most of this optional part consists of changing or extending the code above using regular python/pandas things (no deep learning) so this is accessible to anyone. It will require you to figure out how some of the code does work though, so it's useful in general. \n",
    " \n",
    "Here's what you need to do:\n",
    "* Right now the model trains for 3 epochs total. Increase that number to 5 or more. There's a 3-hour limit per session for GPUs in Great Lakes so if you've completed all of part 1, it's worth getting a fresh session to get all 3 hours again. You can increase the number of epochs if you want too.\n",
    "* When training, we save the last checkpoint (overwriting the previous result) and also see if this is the \"best\" model and save that too. You will need to add more code here to save the model after every epoch. The code to do the saving is already shown in that block, so you'll need to figure out which parts to re-use _and_ be sure to change the directory. Look for \"Optional part 1 goes here\" on where to start\n",
    "* After training completes, let's see how well each of the models does on the test set. We've already provided some code that does the evaluation on the best performing model. Add more code so that it evaluates the models you just saved for each epoch (look for \"Optional part 2 goes here\") and make a plot of the performance on the test set\n",
    "* To measure the impact on NDCG, we'll need to calculate the different bi-encoders' relevance estimates to different files to use in Part 2. You'll need to add more code that loads in each of these models from the checkpoint directories and runs the inference\n",
    "* In the Part 4, just write the different models' predictions to separate files. You might combine this with the part 3 code if it's easier to do there. We haven't marked a spot for it explicitly, but you'll use these files in part 2.\n",
    "\n",
    "The output of Part 1 should be a plot showing the F1 performance per epoch and a list of files for each epoch's trained model's relevance predictions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0069b09b1f1b4decab94c40dfa65f83f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1942718b36684b43af3fcab7b6e26851",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b79aec44f5c8458097e7073dc3506ff2",
      "value": 456318
     }
    },
    "0373b11ff4dc480183e1467dfb44541e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "079031510c1b4b3897c3026ef8dad9a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0acdf0b867cf464dba6d4b18d334e0b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_079031510c1b4b3897c3026ef8dad9a4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_49a1c5c637594df5be80b9480a94d4e6",
      "value": " 25.0/25.0 [00:00&lt;00:00, 258B/s]"
     }
    },
    "1585fa06a3484511b26522604eb8a26b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "181cf860cf834ab9a60ab2806f171933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1942718b36684b43af3fcab7b6e26851": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a719550d8284eee876dfcdef2b2374e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5bbe5bfb4449ab92edcbfc22571eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_545574aa878f4fe0a38e8ba82f3090ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b6b456a89f1a4328b534e68311eafaea",
      "value": " 498/498 [00:00&lt;00:00, 4.34kB/s]"
     }
    },
    "1f67c0c70cdf420b8e0d926f013fbea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fec94f05199469a83ce811f2b75020d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22eae3c5ac864298838da083330c32a4",
       "IPY_MODEL_c8827707070347eca302cca686a70dc8",
       "IPY_MODEL_a8de86d9d8804ddaa05d300b58514dfb"
      ],
      "layout": "IPY_MODEL_69e57f49b7c74fa3a12de04e779b1b90"
     }
    },
    "22eae3c5ac864298838da083330c32a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2c4dd2a706d42e69a34a5fa3042de89",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_87a8a07f75194143b0983e6a9daf226b",
      "value": "Downloading: 100%"
     }
    },
    "249dfe5cb9404ec6a70a476e44d57b55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "275fd704c2d44feba32c75424ba13b7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0cc18ea54f846f5963b305aa53005e8",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac0d62afbe1944bfa83b3ba6a9494f79",
      "value": 25
     }
    },
    "2f77cc2ef3954a90a39a6c313c70db20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93fe0b9515b94a79ba8ac060a57f0edc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8a7783c1125c46ae9e6971449aac7ebe",
      "value": "Downloading: 100%"
     }
    },
    "314614ea95014ec1b45809b2723e9a34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3990eed327e345baab462961fc6aa300": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b86cc6b4cb46455db86a1264a8d1a13d",
       "IPY_MODEL_e89999b1ee3c44358abd66d90297d698",
       "IPY_MODEL_5749880ed5c84d4ca82e0570b6a20fac"
      ],
      "layout": "IPY_MODEL_6ddb7145af3344368239426589e71b4e"
     }
    },
    "3ea5523615eb471b83aa4ecd96354e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3faf2597fe6148e39c8595e9084a84bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40c57f544d894d67a9a672ba40eff578": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f77cc2ef3954a90a39a6c313c70db20",
       "IPY_MODEL_8b26e0836aa9477a98e186b173c9a103",
       "IPY_MODEL_969607d9b3de49beb77b1f8c0a6a633f"
      ],
      "layout": "IPY_MODEL_80f94782f5894afd90f48d32d8577b2e"
     }
    },
    "40fb2b354fd5411db8fa8dafec540c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "428f47d082d548c794c3877be8f92211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb64f05bc8ee4e1faff7b03e726edff5",
       "IPY_MODEL_0069b09b1f1b4decab94c40dfa65f83f",
       "IPY_MODEL_bc0e6ab15cdd4f519d5578af6b7156c9"
      ],
      "layout": "IPY_MODEL_f9ed006a5da3476196ad93c51ad09aee"
     }
    },
    "47380b37dac844f9b1d443efe0e668f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49a1c5c637594df5be80b9480a94d4e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "512640cdd26443d9a980f77c192010d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8686da7f60244bfa8de75ec5f70b26bf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c0082bdb0da74c99bff20110d2f6ed3f",
      "value": "Downloading: 100%"
     }
    },
    "545574aa878f4fe0a38e8ba82f3090ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55a5a37999c44ac3bfc7c3251c1ca5f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5749880ed5c84d4ca82e0570b6a20fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3faf2597fe6148e39c8595e9084a84bd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_40fb2b354fd5411db8fa8dafec540c65",
      "value": " 899k/899k [00:01&lt;00:00, 1.30MB/s]"
     }
    },
    "59b1b791c64247b785355144c183fa09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5af203aa80a14560a69dde3dd99ef665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65a6b7d373664608b2624a16c9d83380": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_512640cdd26443d9a980f77c192010d0",
       "IPY_MODEL_ab410b27569743ce8ad5ff4acd261db5",
       "IPY_MODEL_1e5bbe5bfb4449ab92edcbfc22571eda"
      ],
      "layout": "IPY_MODEL_7b4995a9abfc490cb53576bde244a24e"
     }
    },
    "69e57f49b7c74fa3a12de04e779b1b90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ddb7145af3344368239426589e71b4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f88a17b4c8b4280ae0503b29028290a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75a35c1e924d4857ba36092aeb3a51de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "793bf706fc36491c80ff960d4578f8da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b4995a9abfc490cb53576bde244a24e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80f94782f5894afd90f48d32d8577b2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81381720e36c446085c8a69b79780425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9702b553eabc4de385ac63b5ed340160",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5af203aa80a14560a69dde3dd99ef665",
      "value": "Downloading: 100%"
     }
    },
    "8686da7f60244bfa8de75ec5f70b26bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87a8a07f75194143b0983e6a9daf226b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a7783c1125c46ae9e6971449aac7ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b26e0836aa9477a98e186b173c9a103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1585fa06a3484511b26522604eb8a26b",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47380b37dac844f9b1d443efe0e668f5",
      "value": 150
     }
    },
    "9342bd3e0e8a4005bd181c48e74a79b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fe0b9515b94a79ba8ac060a57f0edc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "969607d9b3de49beb77b1f8c0a6a633f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a719550d8284eee876dfcdef2b2374e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_59b1b791c64247b785355144c183fa09",
      "value": " 150/150 [00:00&lt;00:00, 4.61kB/s]"
     }
    },
    "9702b553eabc4de385ac63b5ed340160": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c344891ac0147f6ac8bef5a94961efb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8de86d9d8804ddaa05d300b58514dfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_249dfe5cb9404ec6a70a476e44d57b55",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_75a35c1e924d4857ba36092aeb3a51de",
      "value": " 499M/499M [00:08&lt;00:00, 57.4MB/s]"
     }
    },
    "ab410b27569743ce8ad5ff4acd261db5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d02158f8d80340b39f7f2c3dbf7d5599",
      "max": 498,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b655c0357a3541f0b78ac2f2fe2ebf27",
      "value": 498
     }
    },
    "ac0d62afbe1944bfa83b3ba6a9494f79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b655c0357a3541f0b78ac2f2fe2ebf27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6b456a89f1a4328b534e68311eafaea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b79aec44f5c8458097e7073dc3506ff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b86cc6b4cb46455db86a1264a8d1a13d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c344891ac0147f6ac8bef5a94961efb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0373b11ff4dc480183e1467dfb44541e",
      "value": "Downloading: 100%"
     }
    },
    "bb7298e53350462fba7054d97483c2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc0e6ab15cdd4f519d5578af6b7156c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f88a17b4c8b4280ae0503b29028290a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9342bd3e0e8a4005bd181c48e74a79b3",
      "value": " 456k/456k [00:00&lt;00:00, 496kB/s]"
     }
    },
    "c0082bdb0da74c99bff20110d2f6ed3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c738f960d8a540b295a85853d6a3b78f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81381720e36c446085c8a69b79780425",
       "IPY_MODEL_275fd704c2d44feba32c75424ba13b7d",
       "IPY_MODEL_0acdf0b867cf464dba6d4b18d334e0b9"
      ],
      "layout": "IPY_MODEL_793bf706fc36491c80ff960d4578f8da"
     }
    },
    "c8827707070347eca302cca686a70dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55a5a37999c44ac3bfc7c3251c1ca5f7",
      "max": 498627950,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_181cf860cf834ab9a60ab2806f171933",
      "value": 498627950
     }
    },
    "d02158f8d80340b39f7f2c3dbf7d5599": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0cc18ea54f846f5963b305aa53005e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2c4dd2a706d42e69a34a5fa3042de89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e89999b1ee3c44358abd66d90297d698": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f67c0c70cdf420b8e0d926f013fbea5",
      "max": 898822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ea5523615eb471b83aa4ecd96354e54",
      "value": 898822
     }
    },
    "eb64f05bc8ee4e1faff7b03e726edff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_314614ea95014ec1b45809b2723e9a34",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bb7298e53350462fba7054d97483c2f7",
      "value": "Downloading: 100%"
     }
    },
    "f9ed006a5da3476196ad93c51ad09aee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
